from langdetect import detect
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# -------------------------
# Config
# -------------------------
INDEX_DIR = "faiss_index"
EMBED_MODEL = "paraphrase-multilingual-MiniLM-L12-v2"

instructions = {
    "en": "Answer ONLY using the medical context below. If the answer is not present, reply 'I donтАЩt know'.",
    "or": "рмХрнГрмкрнЯрм╛ рмирм┐рморнНрмирнЛрм▓рм┐рмЦрм┐рмд рмкрнНрм░рм╕рмЩрнНрмЧрм░рнБ рморм╛рмдрнНрм░ рмЙрмдрнНрмдрм░ рмжрм┐рмЕрмирнНрмдрнБ | рмЙрмдрнНрмдрм░ рмирмерм┐рм▓рнЗ 'рморнБрмБ рмЬрм╛рмгрм┐рмирм┐' рмХрнБрм╣рмирнНрмдрнБ |",  # Odia
    "mr": "рдлрдХреНрдд рдЦрд╛рд▓реАрд▓ рд╕рдВрджрд░реНрдн рд╡рд╛рдкрд░реВрди рдЙрддреНрддрд░ рджреНрдпрд╛. рдЙрддреНрддрд░ рдЙрдкрд▓рдмреНрдз рдирд╕реЗрд▓ рддрд░ 'рдорд▓рд╛ рдорд╛рд╣рд┐рдд рдирд╛рд╣реА' рдЕрд╕реЗ рдореНрд╣рдгрд╛.",  # Marathi
    "ur": "╪╡╪▒┘Б ┘Ж█М┌Ж█Т ╪п█М█Т ┌п╪ж█Т ╪│█М╪з┘В ┘И ╪│╪и╪з┘В ┌й╪з ╪з╪│╪к╪╣┘Е╪з┘Д ┌й╪▒╪к█Т █Б┘И╪ж█Т ╪м┘И╪з╪и ╪п█М┌║█Ф ╪з┌п╪▒ ╪м┘И╪з╪и ┘Е┘И╪м┘И╪п ┘Ж█Б█М┌║ █Б█Т ╪к┘И '┘Е╪м┌╛█Т ┘Ж█Б█М┌║ ┘Е╪╣┘Д┘И┘Е' ┌й█Б█М┌║█Ф",  # Urdu
    "ta": "роХрпАро┤рпЗ роЙро│рпНро│ роЪрпВро┤ро▓рпИ роороЯрпНроЯрпБроорпЗ рокропройрпНрокроЯрпБродрпНродро┐ рокродро┐ро▓ро│ро┐роХрпНроХро╡рпБроорпН. рокродро┐ро▓рпН роЗро▓рпНро▓ро╛ро╡ро┐роЯрпНроЯро╛ро▓рпН 'роОройроХрпНроХрпБ родрпЖро░ро┐ропро╡ро┐ро▓рпНро▓рпИ' роОройрпНро▒рпБ роЪрпКро▓рпНро▓ро╡рпБроорпН.",  # Tamil
    "te": "р░Хр▒Нр░░р░┐р░Вр░ж р░Зр░Ър▒Нр░Ър░┐р░и р░╕р░Вр░жр░░р▒Нр░нр░В р░Жр░зр░╛р░░р░Вр░Чр░╛ р░ор░╛р░др▒Нр░░р░ор▒З р░╕р░ор░╛р░зр░╛р░ир░В р░Зр░╡р▒Нр░╡р░Вр░бр░┐. р░╕р░ор░╛р░зр░╛р░ир░В р░▓р▒Зр░Хр░кр▒Лр░др▒З 'р░ир░╛р░Хр▒Б р░др▒Жр░▓р░┐р░пр░жр▒Б' р░Ер░ир░┐ р░Ър▒Жр░кр▒Нр░кр░Вр░бр░┐.",  # Telugu
}

# -------------------------
# Build RAG pipeline
# -------------------------
def load_vectorstore():
    embed = HuggingFaceEmbeddings(model_name=EMBED_MODEL)
    vect = FAISS.load_local(INDEX_DIR, embed, allow_dangerous_deserialization=True)
    return vect

def build_prompt(query, lang):
    instruction_text = instructions.get(lang, instructions["en"])
    return f"""
You are a multilingual medical assistant.
{instruction_text}
Always answer in the SAME language as the question.

Context:
{{context}}

Question: {query}

Answer:
"""

def build_chain(vect, query, lang):
    llm = Ollama(model="phi3:mini")  # or smaller llama3 variant if GPU is limited
    retriever = vect.as_retriever(search_type="similarity", search_kwargs={"k": 5})
    prompt = PromptTemplate.from_template(build_prompt(query, lang))
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True,
    )
    return qa

# -------------------------
# Interactive Loop
# -------------------------
if __name__ == "__main__":
    vect = load_vectorstore()
    print("тЬЕ Chatbot ready. Type 'exit' to quit.")

    while True:
        query = input("\nЁЯСд You: ")
        if query.lower() in ["exit", "quit"]:
            break

        try:
            lang = detect(query)
        except:
            lang = "en"

        qa = build_chain(vect, query, lang)
        result = qa.invoke({"query": query})

        print("\nЁЯдЦ Bot:", result["result"])
        print("ЁЯУО Sources:", [d.metadata for d in result["source_documents"]])
