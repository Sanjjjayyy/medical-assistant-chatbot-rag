import streamlit as st
from langdetect import detect
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# -------------------------
# Config
# -------------------------
INDEX_DIR = "faiss_index"
EMBED_MODEL = "paraphrase-multilingual-MiniLM-L12-v2"

instructions = {
    "en": "Answer ONLY using the medical context below. If the answer is not present, reply 'I donтАЩt know'.",
    "or": "рмХрнГрмкрнЯрм╛ рмирм┐рморнНрмирнЛрм▓рм┐рмЦрм┐рмд рмкрнНрм░рм╕рмЩрнНрмЧрм░рнБ рморм╛рмдрнНрм░ рмЙрмдрнНрмдрм░ рмжрм┐рмЕрмирнНрмдрнБ | рмЙрмдрнНрмдрм░ рмирмерм┐рм▓рнЗ 'рморнБрмБ рмЬрм╛рмгрм┐рмирм┐' рмХрнБрм╣рмирнНрмдрнБ |",  # Odia
    "mr": "рдлрдХреНрдд рдЦрд╛рд▓реАрд▓ рд╕рдВрджрд░реНрдн рд╡рд╛рдкрд░реВрди рдЙрддреНрддрд░ рджреНрдпрд╛. рдЙрддреНрддрд░ рдЙрдкрд▓рдмреНрдз рдирд╕реЗрд▓ рддрд░ 'рдорд▓рд╛ рдорд╛рд╣рд┐рдд рдирд╛рд╣реА' рдЕрд╕реЗ рдореНрд╣рдгрд╛.",  # Marathi
    "ur": "╪╡╪▒┘Б ┘Ж█М┌Ж█Т ╪п█М█Т ┌п╪ж█Т ╪│█М╪з┘В ┘И ╪│╪и╪з┘В ┌й╪з ╪з╪│╪к╪╣┘Е╪з┘Д ┌й╪▒╪к█Т █Б┘И╪ж█Т ╪м┘И╪з╪и ╪п█М┌║█Ф ╪з┌п╪▒ ╪м┘И╪з╪и ┘Е┘И╪м┘И╪п ┘Ж█Б█М┌║ █Б█Т ╪к┘И '┘Е╪м┌╛█Т ┘Ж█Б█М┌║ ┘Е╪╣┘Д┘И┘Е' ┌й█Б█М┌║█Ф",  # Urdu
    "ta": "роХрпАро┤рпЗ роЙро│рпНро│ роЪрпВро┤ро▓рпИ роороЯрпНроЯрпБроорпЗ рокропройрпНрокроЯрпБродрпНродро┐ рокродро┐ро▓ро│ро┐роХрпНроХро╡рпБроорпН. рокродро┐ро▓рпН роЗро▓рпНро▓ро╛ро╡ро┐роЯрпНроЯро╛ро▓рпН 'роОройроХрпНроХрпБ родрпЖро░ро┐ропро╡ро┐ро▓рпНро▓рпИ' роОройрпНро▒рпБ роЪрпКро▓рпНро▓ро╡рпБроорпН.",  # Tamil
    "te": "р░Хр▒Нр░░р░┐р░Вр░ж р░Зр░Ър▒Нр░Ър░┐р░и р░╕р░Вр░жр░░р▒Нр░нр░В р░Жр░зр░╛р░░р░Вр░Чр░╛ р░ор░╛р░др▒Нр░░р░ор▒З р░╕р░ор░╛р░зр░╛р░ир░В р░Зр░╡р▒Нр░╡р░Вр░бр░┐. р░╕р░ор░╛р░зр░╛р░ир░В р░▓р▒Зр░Хр░кр▒Лр░др▒З 'р░ир░╛р░Хр▒Б р░др▒Жр░▓р░┐р░пр░жр▒Б' р░Ер░ир░┐ р░Ър▒Жр░кр▒Нр░кр░Вр░бр░┐.",  # Telugu
}

# -------------------------
# Helper functions
# -------------------------
def load_vectorstore():
    embed = HuggingFaceEmbeddings(model_name=EMBED_MODEL)
    vect = FAISS.load_local(INDEX_DIR, embed, allow_dangerous_deserialization=True)
    return vect

def build_prompt(query, lang):
    instruction_text = instructions.get(lang, instructions["en"])
    return f"""
You are a multilingual medical assistant.
{instruction_text}
Always answer in the SAME language as the question.

Context:
{{context}}

Question: {query}

Answer:
"""

def get_answer(query, vect):
    try:
        lang = detect(query)
    except:
        lang = "en"

    llm = Ollama(model="phi3:mini")  # тЬЕ lightweight model for frontend
    retriever = vect.as_retriever(search_type="similarity", search_kwargs={"k": 5})
    prompt = PromptTemplate.from_template(build_prompt(query, lang))
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True,
    )
    return qa.invoke({"query": query})

# -------------------------
# Streamlit UI
# -------------------------
st.set_page_config(page_title="ЁЯй║ Medical Assistant Chatbot", layout="wide")

st.title("ЁЯй║ Multilingual Medical Chatbot")
st.markdown("тЪая╕П **Disclaimer:** This chatbot is for informational purposes only. Please consult a doctor for real medical advice.")

vect = load_vectorstore()

user_query = st.text_input("Enter your symptoms or question:")

if st.button("Ask") and user_query.strip():
    with st.spinner("Thinking..."):
        result = get_answer(user_query, vect)
        st.markdown(f"**ЁЯдЦ Bot:** {result['result']}")
        with st.expander("ЁЯУО Sources"):
            for doc in result["source_documents"]:
                st.write(doc.metadata)
